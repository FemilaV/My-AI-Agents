# ğŸ“š Smart Study Buddy (Contextual RAG with Pinecone)

Smart Study Buddy is a **context-aware Retrieval-Augmented Generation (RAG)** system designed to help grade 8 students to study from their PDFs (notes, chapters, textbooks). It enhances each chunk with **LLM-generated context**, stores them in **Pinecone**, and lets you have a **conversational study session** using GPT.

---

## ğŸš€ Features

- ğŸ“„ Load and process multiple PDFs
- âœ‚ï¸ Smart chunking with overlap
- ğŸ§  **Contextual retrieval** (LLM-generated summaries per chunk)
- ğŸ” Vector search using Pinecone
- ğŸ’¬ Conversational Q&A with memory
- ğŸ—‚ï¸ Filter by source or topic
- â™»ï¸ Easy index reset

---

## ğŸ§± Tech Stack

| Component | Used Technology |
|---------|----------------|
| LLM | OpenAI GPT (chat + summarization) |
| Embeddings | OpenAI `text-embedding-3-small` |
| Vector DB | Pinecone (Serverless) |
| Framework | LangChain |
| Files | PDF |

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.py                # Core RAG pipeline + chat interface
â”œâ”€â”€ create_embedding.py    # One-time (or manual) PDF indexing script
â”œâ”€â”€ clear_pinecone.py      # Utility to clear Pinecone index
â”œâ”€â”€ study_materials/       # Folder containing PDF files
â”œâ”€â”€ .env                   # API keys
â””â”€â”€ README.md
```

---

## ğŸ” Environment Setup

Create a `.env` file in the root directory:

```
OPENAI_API_KEY=your_openai_api_key
PINECONE_API_KEY=your_pinecone_api_key
```

Install dependencies:

```
pip install -r requirements.txt
```

---

## ğŸ§  How It Works (High Level)

1. PDFs are loaded and split into chunks
2. Each chunk is sent to GPT to generate **contextual metadata**
3. Context + content is embedded
4. Embeddings are stored in Pinecone
5. User questions retrieve relevant chunks
6. GPT answers using retrieved context

---

## ğŸ§© File Explanations

### 1ï¸âƒ£ `create_embedding.py`

**Purpose:**
- One-time (or occasional) script to process PDFs and upload embeddings

**What it does:**
- Checks if Pinecone index already has vectors
- Skips re-indexing if data exists
- Calls `process_all_pdfs()` from `main.py`
- Waits for Pinecone index propagation

**When to run:**
- First time setup
- After adding new PDFs
- After clearing the index

**Run command:**
```
python create_embedding.py
```

---

### 2ï¸âƒ£ `clear_pinecone.py`

**Purpose:**
- Utility script to completely clear the Pinecone index

**What it does:**
- Connects to Pinecone
- Checks if index exists
- Deletes **all vectors** inside the index

âš ï¸ **Warning:** This does NOT delete the index itself â€” only the data.

**Run command:**
```
python clear_pinecone.py
```

---

### 3ï¸âƒ£ `main.py` (Core Logic)

This is the heart of the project.

#### ğŸ”¹ PDF Processing
- Loads PDFs using `PyPDFLoader`
- Splits text using `RecursiveCharacterTextSplitter`
- Adds metadata (source, type, filename)

#### ğŸ”¹ Contextual Retrieval (Key Feature)
Each chunk is enhanced using GPT:
- Topic of the chunk
- Relation to the document
- Important concepts

This improves retrieval accuracy significantly.

#### ğŸ”¹ Pinecone Setup
- Creates index if missing
- Ensures correct embedding dimension (1536)
- Recreates index if dimension mismatch

#### ğŸ”¹ Conversational RAG
- Uses `ConversationSummaryMemory`
- History-aware retriever
- Maintains context across questions

#### ğŸ”¹ Interactive Study Session
Run:
```
python main.py
```
Then ask questions naturally:
```
You: Explain binary search
You: Give an example
You: How is it different from linear search?
```

---

## ğŸ” Advanced Utilities

### ğŸ”¹ Search by Topic
Retrieve chunks related to a topic:
```python
search_by_topic("Operating Systems")
```

### ğŸ”¹ Filter by Source
Query only notes or a specific file:
```python
filter_by_source("Explain paging", file_type="notes")
```

---

## ğŸ”„ Typical Workflow

1ï¸âƒ£ Add PDFs to `study_materials/`

2ï¸âƒ£ (Optional) Clear old data
```
python clear_pinecone.py
```

3ï¸âƒ£ Create embeddings
```
python create_embedding.py
```

4ï¸âƒ£ Start study session
```
python main.py
```

---

## âš ï¸ Notes & Tips

- Pinecone indexing may take a few seconds to become searchable
- Contextual chunking increases indexing time but improves quality
- Costs depend on OpenAI + Pinecone usage

---

## âœ… Future Improvements

- Web UI (Streamlit / Next.js)
- Multi-user sessions
- Source citations in chat
- Support for non-PDF documents

---

## ğŸ¤ Credits

Built using:
- LangChain
- OpenAI
- Pinecone

Happy studying! ğŸ“

